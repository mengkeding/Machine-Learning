{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam dataset classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import math\n",
    "import csv\n",
    "from pylab import pcolor, show, colorbar, xticks, yticks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the file\n",
    "train_mat = sio.loadmat(\"spam_data.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training_labels', 'test_data', '__globals__', '__header__', '__version__', 'training_data'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understand the data content\n",
    "train_mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23702, 68)\n",
      "(23702,)\n"
     ]
    }
   ],
   "source": [
    "# Get the training data\n",
    "train_data = train_mat['training_data']\n",
    "# Get the training label\n",
    "train_labels = train_mat['training_labels'][0]\n",
    "# Check the amount of data in the training set\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "train_data, train_labels = shuffle(train_data, train_labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set aside 20% training sample as a validation set\n",
    "size = int(math.ceil(train_data.shape[0] * 0.2))\n",
    "validation_index = np.array(random.sample(range(train_data.shape[0]), size))\n",
    "validation_data = train_data[validation_index]\n",
    "validation_labels = train_labels[validation_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4741, 68)\n",
      "(4741,)\n"
     ]
    }
   ],
   "source": [
    "# Verify the validation set size\n",
    "print(validation_data.shape )\n",
    "print(validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18961, 68)\n",
      "(18961,)\n"
     ]
    }
   ],
   "source": [
    "# Construct the training set\n",
    "training_data = np.delete(train_data, validation_index, 0)\n",
    "training_labels = np.delete(train_labels, validation_index, 0)\n",
    "print(training_data.shape)\n",
    "print(training_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the test data\n",
    "test_data = train_mat['test_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the given data\n",
    "def normalize_data(data):\n",
    "    normalized_data = data/np.linalg.norm(data).reshape((-1, 1))\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "norm_validation_data = normalize_data(validation_data)\n",
    "\n",
    "norm_training_data = normalize_data(training_data)\n",
    "\n",
    "norm_test_data = normalize_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_size = training_data.shape[1]\n",
    "classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(training_data, training_labels, DA_fn, k_fold, constant):\n",
    "    \"\"\"\n",
    "    Do the k-fold cross_validation with given discriminate analysis function\n",
    "    and the constant that modify the covariance matrix\n",
    "    It returns the average validation accuracy with the given constant\n",
    "    \"\"\"\n",
    "    # Partition the data and labels into k-fold\n",
    "    data_set = np.array(np.split(training_data, k_fold))\n",
    "    labels_set = np.array(np.split(training_labels, k_fold))\n",
    "    total_accuracy = []\n",
    "    \n",
    "    for k in range(k_fold):\n",
    "        train_data = np.delete(data_set, k, 0)\n",
    "        train_labels = np.delete(labels_set, k, 0)\n",
    "        validation_data = data_set[k]\n",
    "        validation_labels = labels_set[k]\n",
    "        validation_prediction = DA_fn(train_data, train_labels, validation_data, constant)\n",
    "        total_accuracy.append(np.sum(validation_prediction == validation_labels)/float(len(validation_labels)))\n",
    "    accuracy = sum(total_accuracy)/len(total_accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_const(training_data, training_labels, DA_fn, k_fold, constant_list):\n",
    "    \"\"\"\n",
    "    Find the best constant that results in a greater accuracy in the constant list\n",
    "    \"\"\"\n",
    "    max_accuracy = 0\n",
    "    best_const = 0\n",
    "    \n",
    "    for const in constant_list:\n",
    "        print(\"Testing const\", const)\n",
    "        current_accuracy = cross_validation(training_data, training_labels, DA_fn, k_fold, const)\n",
    "        if current_accuracy > max_accuracy:\n",
    "            max_accuracy = current_accuracy\n",
    "            best_const = const\n",
    "    return best_const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior of  0  is  0.492009915089\n",
      "Prior of  1  is  0.507990084911\n"
     ]
    }
   ],
   "source": [
    "# Calculate the prior probability\n",
    "sample_size = float(training_labels.shape[0])\n",
    "prior_prob = [np.sum(training_labels == c)/sample_size for c in range(classes)]\n",
    "for i, p in enumerate(prior_prob):\n",
    "    print(\"Prior of \", i, \" is \", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LDA(training_data, training_labels, prediction_data, constant):\n",
    "    # Compute the mean\n",
    "    means = [np.mean(training_data[training_labels == c], axis=0) for c in range(classes)]\n",
    "    # Compute the covariance matrix\n",
    "    covariance = [np.cov(training_data[training_labels == c].T) for c in range(classes)]\n",
    "    avg_cov = np.mean(covariance, axis=0) + np.eye(feature_size) * constant# the average covariance matrix of the 10 classes\n",
    "    avg_cov_inv = np.linalg.inv(avg_cov) \n",
    "    \n",
    "    # use the linear discriminant function to make prediction\n",
    "    predictions = [means[c].dot(avg_cov_inv).dot(prediction_data.T) - \\\n",
    "                   0.5 * means[c].T.dot(avg_cov_inv).dot(means[c]) + \\\n",
    "                   np.log(prior_prob[c]) for c in range(classes)]\n",
    "    max_prediction = np.argmax(predictions, axis=0)\n",
    "    return max_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing const 1e-10\n",
      "Testing const 1e-09\n",
      "Testing const 1e-08\n",
      "Testing const 1e-07\n",
      "Testing const 1e-06\n",
      "Testing const 1e-05\n",
      "Testing const 0.0001\n",
      "Testing const 0.001\n",
      "Best constant is 1e-10\n"
     ]
    }
   ],
   "source": [
    "constant_list = np.array([10**i for i in range(-10, -2)])\n",
    "k_fold = 15\n",
    "LDA_best_const = find_best_const(norm_training_data[:18960], training_labels[:18960], LDA, k_fold, constant_list)\n",
    "print(\"Best constant is\", LDA_best_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juntan/anaconda3/envs/datascience/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/Users/juntan/anaconda3/envs/datascience/lib/python3.5/site-packages/numpy/core/_methods.py:68: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/juntan/anaconda3/envs/datascience/lib/python3.5/site-packages/numpy/lib/function_base.py:2487: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning)\n",
      "/Users/juntan/anaconda3/envs/datascience/lib/python3.5/site-packages/numpy/lib/function_base.py:2496: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  c *= 1. / np.float64(fact)\n",
      "/Users/juntan/anaconda3/envs/datascience/lib/python3.5/site-packages/numpy/lib/function_base.py:2496: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= 1. / np.float64(fact)\n"
     ]
    }
   ],
   "source": [
    "training_sizes = [100, 200, 500, 1000, 2000, 5000, 10000, norm_training_data.shape[0]]\n",
    "LDA_validation_error = []\n",
    "for size in training_sizes:\n",
    "    LDA_validation_prediction = LDA(norm_training_data[:size], training_labels[:size],\\\n",
    "                                    norm_validation_data, LDA_best_const)\n",
    "    error_rate = np.sum(LDA_validation_prediction != validation_labels)/float(len(validation_labels))\n",
    "    LDA_validation_error.append(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.50263657456232858,\n",
       " 0.50263657456232858,\n",
       " 0.50263657456232858,\n",
       " 0.50263657456232858,\n",
       " 0.50263657456232858,\n",
       " 0.50263657456232858,\n",
       " 0.15587428812486817,\n",
       " 0.13182872811643112]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_validation_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LDA to classify the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA_predictions = LDA(norm_training_data, training_labels, norm_test_data, LDA_best_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write predictions to the output file\n",
    "i = 0\n",
    "with open('LDA_spam_submission.csv', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['Id'] + ['Category'])\n",
    "    for num in LDA_predictions:\n",
    "        writer.writerow([i] + [num])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LDA, the Kaggle score is 0.85280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDA Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def QDA(training_data, training_labels, prediction_data, constant):\n",
    "    # Compute the mean\n",
    "    classes = 2\n",
    "    expand_size = len(prediction_data)\n",
    "    means = [np.mean(training_data[training_labels == c], axis=0) for c in range(classes)]\n",
    "    # Transform the means into a list of matrix that with each row is a replica of mean\n",
    "    means_transformed = [np.tile(mean.reshape(feature_size, 1), expand_size).T for mean in means]\n",
    "    # Compute the covariance matrix\n",
    "    covariances = [np.cov(training_data[training_labels == c].T) + np.eye(feature_size) * constant\\\n",
    "                   for c in range(classes)]\n",
    "    cov_invs = [np.linalg.inv(cov) for cov in covariances] # inverse of covariance matrix\n",
    "    \n",
    "    # use the discriminant function to make prediction\n",
    "    predictions = [-0.5 * (prediction_data - means_transformed[c]).dot(cov_invs[c])\\\n",
    "                   .dot((prediction_data - means_transformed[c]).T).diagonal() - \\\n",
    "                   0.5 * np.tile(np.log(np.linalg.norm(cov_invs[c])), expand_size) + \\\n",
    "                   np.tile(np.log(prior_prob[c]), expand_size) for c in range(classes)]\n",
    "    max_prediction = np.argmax(predictions, axis=0)\n",
    "    return max_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing const 1e-20\n",
      "Testing const 1e-19\n",
      "Testing const 1e-18\n",
      "Testing const 1e-17\n",
      "Testing const 1e-16\n",
      "Testing const 1e-15\n",
      "Testing const 1e-14\n",
      "Testing const 1e-13\n",
      "Testing const 1e-12\n",
      "Testing const 1e-11\n",
      "Testing const 1e-10\n",
      "Testing const 1e-09\n",
      "Testing const 1e-08\n",
      "Testing const 1e-07\n",
      "Testing const 1e-06\n",
      "Testing const 1e-05\n",
      "Testing const 0.0001\n",
      "Testing const 0.001\n",
      "Best constant is 1e-10\n"
     ]
    }
   ],
   "source": [
    "constant_list = np.array([10**i for i in range(-20, -2)])\n",
    "k_fold = 15\n",
    "QDA_best_const = find_best_const(norm_training_data[:18960], training_labels[:18960], QDA, k_fold, constant_list)\n",
    "print(\"Best constant is\", LDA_best_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juntan/anaconda3/envs/datascience/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/Users/juntan/anaconda3/envs/datascience/lib/python3.5/site-packages/numpy/core/_methods.py:68: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/juntan/anaconda3/envs/datascience/lib/python3.5/site-packages/numpy/lib/function_base.py:2487: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning)\n",
      "/Users/juntan/anaconda3/envs/datascience/lib/python3.5/site-packages/numpy/lib/function_base.py:2496: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  c *= 1. / np.float64(fact)\n",
      "/Users/juntan/anaconda3/envs/datascience/lib/python3.5/site-packages/numpy/lib/function_base.py:2496: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= 1. / np.float64(fact)\n"
     ]
    }
   ],
   "source": [
    "QDA_validation_error = []\n",
    "for size in training_sizes:\n",
    "    QDA_validation_prediction = QDA(norm_training_data[:size], training_labels[:size],\\\n",
    "                                    norm_validation_data, QDA_best_const)\n",
    "    error_rate = np.sum(QDA_validation_prediction != validation_labels)/float(len(validation_labels))\n",
    "    QDA_validation_error.append(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.50263657456232858,\n",
       " 0.50263657456232858,\n",
       " 0.50263657456232858,\n",
       " 0.50263657456232858,\n",
       " 0.50263657456232858,\n",
       " 0.50263657456232858,\n",
       " 0.17274836532377136,\n",
       " 0.15756169584475849]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QDA_validation_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use QDA to classify the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QDA_predictions = QDA(norm_training_data, training_labels, norm_test_data, QDA_best_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write predictions to the output file\n",
    "i = 0\n",
    "with open('QDA_spam_submission.csv', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['Id'] + ['Category'])\n",
    "    for num in QDA_predictions:\n",
    "        writer.writerow([i] + [num])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using QDA, the classification score is 0.83540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
